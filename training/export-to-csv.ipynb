{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "kafkaBrokers='logging-kafka-kafka-kafkaroute-bootstrap-kafka.apps.cluster-7msqx.dynamic.redhatworkshops.io:443'\n",
    "caRootLocation='/workspaces/openshift-lab/ai/training/ssl/CARoot.pem'\n",
    "certLocation='/workspaces/openshift-lab/ai/training/ssl/certificate.pem'\n",
    "keyLocation='/workspaces/openshift-lab/ai/training/ssl/key.pem'\n",
    "password='c97xOn13878b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture my app logs into CSV\n",
    "I am doing this for ease of data manipulation during model training. Once model train is done it will be real time query for 24hrs worth of log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset: 257 end_offset: 2222\n",
      "offset: 557 end_offset: 2222\n",
      "offset: 373 end_offset: 2282\n",
      "offset: 673 end_offset: 2282\n",
      "offset: 441 end_offset: 2258\n",
      "offset: 775 end_offset: 2282\n",
      "offset: 1075 end_offset: 2282\n",
      "offset: 862 end_offset: 2222\n",
      "offset: 1162 end_offset: 2222\n",
      "offset: 6198 end_offset: 8020\n",
      "offset: 5917 end_offset: 7901\n",
      "offset: 6217 end_offset: 7901\n",
      "offset: 6035 end_offset: 8062\n",
      "offset: 6335 end_offset: 8062\n",
      "offset: 6155 end_offset: 7896\n",
      "offset: 1400 end_offset: 3366\n",
      "offset: 1700 end_offset: 3366\n",
      "offset: 1556 end_offset: 3431\n",
      "offset: 1856 end_offset: 3431\n",
      "offset: 1674 end_offset: 3526\n",
      "offset: 755 end_offset: 2258\n",
      "offset: 1055 end_offset: 2258\n",
      "offset: 1375 end_offset: 2222\n",
      "offset: 1675 end_offset: 2222\n",
      "offset: 1492 end_offset: 2282\n",
      "offset: 1909 end_offset: 3366\n",
      "offset: 2209 end_offset: 3366\n",
      "offset: 2065 end_offset: 3431\n",
      "offset: 2365 end_offset: 3431\n",
      "offset: 1463 end_offset: 2258\n",
      "offset: 1783 end_offset: 2222\n",
      "offset: 2083 end_offset: 2222\n",
      "offset: 6567 end_offset: 7901\n",
      "offset: 6867 end_offset: 7901\n",
      "offset: 6685 end_offset: 8062\n",
      "offset: 6552 end_offset: 8020\n",
      "offset: 6852 end_offset: 8020\n",
      "offset: 1897 end_offset: 2258\n",
      "offset: 2197 end_offset: 2258\n",
      "offset: 2034 end_offset: 2282\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "topic='ocplogs-myapp'\n",
    "consumer = KafkaConsumer(topic,\n",
    "    bootstrap_servers=[kafkaBrokers],\n",
    "    value_deserializer=lambda x: x.decode('utf-8'),\n",
    "    security_protocol='SSL',\n",
    "    ssl_check_hostname=False,\n",
    "    ssl_cafile=caRootLocation,\n",
    "    ssl_certfile=certLocation,\n",
    "    # ssl_keyfile=keyLocation,\n",
    "    ssl_password=password,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=False,\n",
    "    consumer_timeout_ms=180000 # 3 minutes\n",
    ")\n",
    "\n",
    "timestamp_24hrs_ago = int((datetime.now() - timedelta(days=1)).timestamp() * 1000)\n",
    "\n",
    "# Get the partitions for the topic\n",
    "partitions = consumer.partitions_for_topic(topic)\n",
    "topic_partitions = [TopicPartition(topic, p) for p in partitions]\n",
    "\n",
    "# Get the offsets for the timestamp 24 hours ago\n",
    "offsets = consumer.offsets_for_times({tp: timestamp_24hrs_ago for tp in topic_partitions})\n",
    "\n",
    "# Seek to the calculated offsets\n",
    "for tp in topic_partitions:\n",
    "    if offsets[tp] is not None:\n",
    "        consumer.seek(tp, offsets[tp].offset)\n",
    "\n",
    "end_offsets = consumer.end_offsets(topic_partitions)\n",
    "\n",
    "data = []\n",
    "count=0\n",
    "for event in consumer:\n",
    "    tp = TopicPartition(event.topic, event.partition)\n",
    "    if event.offset >= end_offsets[tp]:\n",
    "        break\n",
    "    if count>30000:\n",
    "        break\n",
    "    msg = event.value\n",
    "    if msg is None:\n",
    "        print(\"Waiting...\")\n",
    "    else:\n",
    "        obj = json.loads(msg)\n",
    "        timestamp = obj.get('@timestamp', 'N/A')\n",
    "        namespace_name = obj.get('kubernetes', {}).get('namespace_name', 'N/A')\n",
    "        app_name = obj.get('kubernetes', {}).get('container_name', 'N/A')\n",
    "        message = obj.get('message', 'N/A')\n",
    "        if pd.isna(message) or message.startswith('\\t'):\n",
    "            continue\n",
    "        level = obj.get('level', 'N/A')\n",
    "        log_type = obj.get('log_type', 'N/A')\n",
    "        data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'namespace_name': namespace_name,\n",
    "            'app_name': app_name,\n",
    "            'level': level,\n",
    "            'log_type': log_type,\n",
    "            'message': message,\n",
    "            'target_classification': '',\n",
    "        })\n",
    "        # print(f\"Namespace: {namespace_name}, app: {app_name}, Level: {level}, Type: {log_type}, Message: |{message}|\")\n",
    "        # if app_name != 'kafka':\n",
    "        #     # print(f\"level: {level}, count: {count}\")\n",
    "        #     count = count + 0 # do nothing\n",
    "        count+=1\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# now fill up target classification the best way possible to prepare training data\n",
    "\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == \"I am busybox. Running normally..\" else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == 'invoice generated successfully' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == 'backup completed' else row['target_classification'], axis=1)\n",
    "\n",
    "\n",
    "df['target_classification'] = df['message'].apply(lambda x: 'error' if x == \"I am busybox. I have just faced an issue...\" else '')\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if row['message'] == \"RAID array experienced disk crash\" else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if re.match(r\"Lead conversion failed for prospect ID \\d{4} due to missing contact information\\.\", row['message']) else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if re.match(r\"Escalation rule execution failed for ticket ID \\d{4} - undefined escalation level\\.\", row['message']) else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if re.match(r\"Customer follow-up process for lead ID \\d{4} failed due to missing next action\\.\", row['message']) else row['target_classification'], axis=1)\n",
    "\n",
    "df['target_classification'] = df.apply(lambda row: 'security alert' if 'suspicious activity observed from ip' in row['message'] else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'security alert' if row['message'] == 'security breach occurred. user tried to log in from outside of business hours' else row['target_classification'], axis=1)\n",
    "\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == \"The 'ExportToCSV' feature is outdated. Please migrate to 'ExportToXLSX' by the end of Q3.\" else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == 'Support for legacy authentication methods will be discontinued after 2025-06-01.' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == 'Support for legacy authentication methods will be discontinued after 2025-06-01.' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == \"API endpoint 'getCustomerDetails' is deprecated and will be removed in version 3.2. Use 'fetchCustomerInfo' instead.\" else row['target_classification'], axis=1)\n",
    "\n",
    "\n",
    "df.to_csv('myapp_logs.nogit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>namespace_name</th>\n",
       "      <th>app_name</th>\n",
       "      <th>level</th>\n",
       "      <th>log_type</th>\n",
       "      <th>message</th>\n",
       "      <th>target_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-09T04:53:12.143530588Z</td>\n",
       "      <td>myapp</td>\n",
       "      <td>busybox</td>\n",
       "      <td>default</td>\n",
       "      <td>application</td>\n",
       "      <td>I am busybox. Running normally..</td>\n",
       "      <td>info</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-09T04:56:37.451478037Z</td>\n",
       "      <td>myapp</td>\n",
       "      <td>billing</td>\n",
       "      <td>default</td>\n",
       "      <td>application</td>\n",
       "      <td>security breach occurred. user tried to log in...</td>\n",
       "      <td>security alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-09T04:56:50.646327206Z</td>\n",
       "      <td>myapp</td>\n",
       "      <td>legacycrm</td>\n",
       "      <td>default</td>\n",
       "      <td>application</td>\n",
       "      <td>The 'ExportToCSV' feature is outdated. Please ...</td>\n",
       "      <td>warning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-09T04:57:20.651643036Z</td>\n",
       "      <td>myapp</td>\n",
       "      <td>legacycrm</td>\n",
       "      <td>default</td>\n",
       "      <td>application</td>\n",
       "      <td>API endpoint 'getCustomerDetails' is deprecate...</td>\n",
       "      <td>warning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-09T04:57:47.467350244Z</td>\n",
       "      <td>myapp</td>\n",
       "      <td>billing</td>\n",
       "      <td>default</td>\n",
       "      <td>application</td>\n",
       "      <td>suspicious activity observed from ip 10.2.4.5</td>\n",
       "      <td>security alert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp namespace_name   app_name    level  \\\n",
       "0  2025-03-09T04:53:12.143530588Z          myapp    busybox  default   \n",
       "1  2025-03-09T04:56:37.451478037Z          myapp    billing  default   \n",
       "2  2025-03-09T04:56:50.646327206Z          myapp  legacycrm  default   \n",
       "3  2025-03-09T04:57:20.651643036Z          myapp  legacycrm  default   \n",
       "4  2025-03-09T04:57:47.467350244Z          myapp    billing  default   \n",
       "\n",
       "      log_type                                            message  \\\n",
       "0  application                   I am busybox. Running normally..   \n",
       "1  application  security breach occurred. user tried to log in...   \n",
       "2  application  The 'ExportToCSV' feature is outdated. Please ...   \n",
       "3  application  API endpoint 'getCustomerDetails' is deprecate...   \n",
       "4  application      suspicious activity observed from ip 10.2.4.5   \n",
       "\n",
       "  target_classification  \n",
       "0                  info  \n",
       "1        security alert  \n",
       "2               warning  \n",
       "3               warning  \n",
       "4        security alert  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('/workspaces/openshift-lab/ai/model/myapp_logs.nogit.csv')\n",
    "\n",
    "# Initialize the target_classification column with empty strings\n",
    "df['target_classification'] = ''\n",
    "\n",
    "# Apply the classification logic\n",
    "\n",
    "df['target_classification'] = df['message'].apply(lambda x: 'info' if x == \"I am busybox. Running normally..\" else '')\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == 'invoice generated successfully' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == 'backup completed' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == 'application is up and running within acceptable parameters' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'info' if row['message'] == 'invoice generated successfully' else row['target_classification'], axis=1)\n",
    "\n",
    "df['target_classification'] = df['message'].apply(lambda x: 'error' if x == \"I am busybox. I have just faced an issue...\" else '')\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if row['message'] == \"RAID array experienced disk crash\" else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if re.match(r\"Lead conversion failed for prospect ID \\d{4} due to missing contact information\\.\", row['message']) else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if re.match(r\"Escalation rule execution failed for ticket ID \\d{4} - undefined escalation level\\.\", row['message']) else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'error' if re.match(r\"Customer follow-up process for lead ID \\d{4} failed due to missing next action\\.\", row['message']) else row['target_classification'], axis=1)\n",
    "\n",
    "df['target_classification'] = df.apply(lambda row: 'security alert' if 'suspicious activity observed from ip' in row['message'] else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'security alert' if row['message'] == 'security breach occurred. user tried to log in from outside of business hours' else row['target_classification'], axis=1)\n",
    "\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == \"The 'ExportToCSV' feature is outdated. Please migrate to 'ExportToXLSX' by the end of Q3.\" else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == 'Support for legacy authentication methods will be discontinued after 2025-06-01.' else row['target_classification'], axis=1)\n",
    "df['target_classification'] = df.apply(lambda row: 'warning' if row['message'] == \"API endpoint 'getCustomerDetails' is deprecated and will be removed in version 3.2. Use 'fetchCustomerInfo' instead.\" else row['target_classification'], axis=1)\n",
    "\n",
    "\n",
    "# this is a hack. I have no idea why the \"info\" (as written above) block did not work \n",
    "# I inspected the data and looks like other that info every other row has taregt_classification fill out. \n",
    "# so I jumped the conclusion that the info block did not work and I am filling whatever is empty with info.\n",
    "df['target_classification'] = df['target_classification'].apply(lambda x: 'info' if x == '' else x)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('/workspaces/openshift-lab/ai/model/myapp_logs_with_classification.nogit.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture application logs into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/pandas/__init__.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m     28\u001b[39m     )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/pandas/compat/__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_constants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     IS64,\n\u001b[32m     19\u001b[39m     ISMUSL,\n\u001b[32m     20\u001b[39m     PY310,\n\u001b[32m     21\u001b[39m     PY311,\n\u001b[32m     22\u001b[39m     PY312,\n\u001b[32m     23\u001b[39m     PYPY,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompressors\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ISMUSL' from 'pandas.compat._constants' (/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/pandas/compat/_constants.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m topic=\u001b[33m'\u001b[39m\u001b[33mocplogs-application\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m consumer = KafkaConsumer(topic,\n\u001b[32m      5\u001b[39m     bootstrap_servers=[kafkaBrokers],\n\u001b[32m      6\u001b[39m     value_deserializer=\u001b[38;5;28;01mlambda\u001b[39;00m x: x.decode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     enable_auto_commit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/pandas/__init__.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     30\u001b[39m     _module = _err.name\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not built. If you want to import \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpandas from the source directory, you may need to run \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython setup.py build_ext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to build the C extensions first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_err\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     get_option,\n\u001b[32m     39\u001b[39m     set_option,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     options,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "topic='ocplogs-application'\n",
    "consumer = KafkaConsumer(topic,\n",
    "    bootstrap_servers=[kafkaBrokers],\n",
    "    value_deserializer=lambda x: x.decode('utf-8'),\n",
    "    security_protocol='SSL',\n",
    "    ssl_check_hostname=False,\n",
    "    ssl_cafile=caRootLocation,\n",
    "    ssl_certfile=certLocation,\n",
    "    # ssl_keyfile=keyLocation,\n",
    "    ssl_password=password,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    ")\n",
    "data = []\n",
    "count=0\n",
    "for event in consumer:\n",
    "    if count>8000:\n",
    "        break\n",
    "    msg = event.value\n",
    "    if msg is None:\n",
    "        print(\"Waiting...\")\n",
    "    else:\n",
    "        obj = json.loads(msg)\n",
    "        timestamp = obj.get('@timestamp', 'N/A')\n",
    "        namespace_name = obj.get('kubernetes', {}).get('namespace_name', 'N/A')\n",
    "        app_name = obj.get('kubernetes', {}).get('container_name', 'N/A')\n",
    "        message = obj.get('message', 'N/A')\n",
    "        if pd.isna(message) or message.startswith('\\t'):\n",
    "            continue\n",
    "        level = obj.get('level', 'N/A')\n",
    "        log_type = obj.get('log_type', 'N/A')\n",
    "        data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'namespace_name': namespace_name,\n",
    "            'app_name': app_name,\n",
    "            'level': level,\n",
    "            'log_type': log_type,\n",
    "            'message': message\n",
    "        })\n",
    "        # print(f\"Namespace: {namespace_name}, app: {app_name}, Level: {level}, Type: {log_type}, Message: |{message}|\")\n",
    "        # if app_name != 'kafka':\n",
    "        #     # print(f\"level: {level}, count: {count}\")\n",
    "        #     count = count + 0 # do nothing\n",
    "        count+=1\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('application_logs.nogit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture audit logs in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic='ocplogs-audit'\n",
    "consumer = KafkaConsumer(topic,\n",
    "    bootstrap_servers=[kafkaBrokers],\n",
    "    value_deserializer=lambda x: x.decode('utf-8'),\n",
    "    security_protocol='SSL',\n",
    "    ssl_check_hostname=False,\n",
    "    ssl_cafile=caRootLocation,\n",
    "    ssl_certfile=certLocation,\n",
    "    # ssl_keyfile=keyLocation,\n",
    "    ssl_password=password,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    ")\n",
    "\n",
    "data = []\n",
    "count=0\n",
    "for event in consumer:\n",
    "    if count>10000:\n",
    "        break\n",
    "    msg = event.value\n",
    "    if msg is None:\n",
    "        print(\"Waiting...\")\n",
    "    else:\n",
    "        obj = json.loads(msg)\n",
    "        timestamp = obj.get('@timestamp', 'N/A')\n",
    "        kind = obj.get('kind', 'N/A')\n",
    "        level = obj.get('level', 'N/A')\n",
    "        source_ips = ', '.join(obj.get('sourceIPs', []))\n",
    "        log_source = obj.get('log_source', 'N/A')\n",
    "        log_type = obj.get('log_type', 'N/A')\n",
    "        user_name = obj.get('user', {}).get('username', 'N/A')\n",
    "        data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'kind': kind,\n",
    "            'level': level,\n",
    "            'user_name': user_name,\n",
    "            'log_type': log_type,\n",
    "            'log_source': log_source,            \n",
    "            'source_ips': source_ips,            \n",
    "        })\n",
    "        count+=1\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('audit_logs.nogit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# capture infra logs into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic='ocplogs-infrastructure'\n",
    "consumer = KafkaConsumer(topic,\n",
    "    bootstrap_servers=[kafkaBrokers],\n",
    "    value_deserializer=lambda x: x.decode('utf-8'),\n",
    "    security_protocol='SSL',\n",
    "    ssl_check_hostname=False,\n",
    "    ssl_cafile=caRootLocation,\n",
    "    ssl_certfile=certLocation,\n",
    "    # ssl_keyfile=keyLocation,\n",
    "    ssl_password=password,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    ")\n",
    "\n",
    "data = []\n",
    "count=0\n",
    "for event in consumer:\n",
    "    if count>10000:\n",
    "        break\n",
    "    msg = event.value\n",
    "    if msg is None:\n",
    "        print(\"Waiting...\")\n",
    "    else:\n",
    "        obj = json.loads(msg)\n",
    "        print(obj)\n",
    "        timestamp = obj.get('@timestamp', 'N/A')\n",
    "        runtime_scope = obj.get('_RUNTIME_SCOPE', 'N/A')\n",
    "        level = obj.get('level', 'N/A')\n",
    "        log_source = obj.get('log_source', 'N/A')\n",
    "        log_type = obj.get('log_type', 'N/A')\n",
    "        tag = obj.get('tag', 'N/A')\n",
    "        message = obj.get('message', 'N/A')\n",
    "        data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'runtime_scope': runtime_scope,\n",
    "            'level': level,\n",
    "            'log_type': log_type,\n",
    "            'log_source': log_source,\n",
    "            'tag': tag,\n",
    "            'message': message                  \n",
    "        })\n",
    "        count+=1\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('infra_logs.nogit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
