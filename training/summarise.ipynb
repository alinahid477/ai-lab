{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ai-lab/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      5\u001b[39m max_model_len, tp_size = \u001b[32m4096\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m      6\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mneuralmagic/granite-3.1-2b-base-quantized.w4a16\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "max_model_len, tp_size = 4096, 1\n",
    "model_name = \"neuralmagic/granite-3.1-2b-base-quantized.w4a16\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, device='cpu')\n",
    "sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = [\n",
    "    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n",
    "]\n",
    "\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "generated_text = [output.outputs[0].text for output in outputs]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files:  40%|████      | 4/10 [00:23<00:35,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_path = \"ibm-granite/granite-3.1-1b-a400m-base\"\n",
    "# model_path = \"DevQuasar/ibm-granite.granite-3.1-1b-a400m-instruct-GGUF\"\n",
    "snapshot_download(repo_id=model_path, repo_type=\"model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "class SecurityEvent(BaseModel):\n",
    "    # The reasoning for why this event is relevant.\n",
    "    reasoning: str\n",
    "\n",
    "    # The type of event.\n",
    "    event_type: str\n",
    "\n",
    "    # Whether this event requires human review.\n",
    "    requires_human_review: bool\n",
    "\n",
    "    # The confidence score for this event. I'm not sure if this\n",
    "    # is meaningful for language models, but it's here if we want it.\n",
    "    confidence_score: float = Field(\n",
    "        ge=0.0, \n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )\n",
    "\n",
    "    # Recommended actions for this event.\n",
    "    recommended_actions: list[str]\n",
    "\n",
    "class HardwareFailureEvent(BaseModel):\n",
    "    # The reasoning for why this event is relevant.\n",
    "    reasoning: str\n",
    "\n",
    "    # The type of event.\n",
    "    event_type: str\n",
    "\n",
    "    # Whether this event requires human review.\n",
    "    requires_human_review: bool\n",
    "\n",
    "    # The confidence score for this event. I'm not sure if this\n",
    "    # is meaningful for language models, but it's here if we want it.\n",
    "    confidence_score: float = Field(\n",
    "        ge=0.0, \n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )\n",
    "\n",
    "    # Recommended actions for this event.\n",
    "    recommended_actions: list[str]\n",
    "\n",
    "class LogAnalysis(BaseModel):\n",
    "    # A summary of the analysis.\n",
    "    summary: str\n",
    "\n",
    "    # Observations about the logs.\n",
    "    observations: list[str]\n",
    "\n",
    "    # Planning for the analysis.\n",
    "    planning: list[str]\n",
    "\n",
    "    # Security events found in the logs.\n",
    "    security_events: list[SecurityEvent]\n",
    "\n",
    "    # harware failure events found in the logs.\n",
    "    Hardware_failure_events: list[HardwareFailureEvent]\n",
    "\n",
    "    # # Traffic patterns found in the logs.\n",
    "    # traffic_patterns: list[WebTrafficPattern]\n",
    "\n",
    "    # # The highest severity event found.\n",
    "    # highest_severity: Optional[SeverityLevel]\n",
    "    requires_immediate_attention: bool\n",
    "\n",
    "\n",
    "class CommandParameter(BaseModel):\n",
    "    # parameter name.\n",
    "    name: Optional[str]\n",
    "\n",
    "    # parameter valye.\n",
    "    value: Optional[str]\n",
    "\n",
    "class SentenceAnalysis(BaseModel):\n",
    "    # the intended command in the english sentence\n",
    "    command: str\n",
    "\n",
    "    # paramteres found in the english for the command\n",
    "    parameters: Optional[list[CommandParameter]]\n",
    "\n",
    "    # The confidence score for this event. I'm not sure if this\n",
    "    # is meaningful for language models, but it's here if we want it.\n",
    "    confidence_score: float = Field(\n",
    "        ge=0.0, \n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5519, 0.6538, 0.1946],\n",
      "        [0.8922, 0.9771, 0.0199],\n",
      "        [0.1398, 0.2076, 0.8230],\n",
      "        [0.1944, 0.3466, 0.6608],\n",
      "        [0.9662, 0.0243, 0.7986]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraniteMoeForCausalLM(\n",
       "  (model): GraniteMoeModel(\n",
       "    (embed_tokens): Embedding(49152, 1024, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GraniteMoeDecoderLayer(\n",
       "        (self_attn): GraniteMoeSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (block_sparse_moe): GraniteMoeMoE(\n",
       "          (activation): SiLU()\n",
       "          (input_linear): GraniteMoeParallelExperts()\n",
       "          (output_linear): GraniteMoeParallelExperts()\n",
       "          (router): GraniteMoeTopKGating(\n",
       "            (layer): Linear(in_features=1024, out_features=32, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): GraniteMoeRMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): GraniteMoeRMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GraniteMoeRMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): GraniteMoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.1-1b-a400m-base\"\n",
    "# model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_path = \"ibm-granite/granite-3.1-3b-a800m-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# drop device_map if running on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGID-0 2025-03-09T04:52:20.600177813Z application: legacycrm in namespace: myapp threw warning: The 'ExportToCSV' feature is outdated. Please migrate to 'ExportToXLSX' by the end of Q3.\n",
      "LOGID-1 2025-03-09T04:53:17.407427133Z application: billing in namespace: myapp output info: application is up and running within acceptable parameters\n",
      "LOGID-2 2025-03-09T04:59:10.669781089Z application: legacycrm in namespace: myapp threw warning: Support for legacy authentication methods will be discontinued after 2025-06-01.\n",
      "LOGID-3 2025-03-09T04:59:20.671493491Z application: legacycrm in namespace: myapp threw warning: Support for legacy authentication methods will be discontinued after 2025-06-01.\n",
      "LOGID-4 2025-03-09T04:59:40.674811633Z application: legacycrm in namespace: myapp threw warning: API endpoint 'getCustomerDetails' is deprecated and will be removed in version 3.2. Use 'fetchCustomerInfo' instead.\n",
      "{'input_ids': tensor([[4282,  884,  600,  ...,   69, 7082,   48]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# change input text as desired\n",
    "chat = \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\"\n",
    "# chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# tokenize the text\n",
    "\n",
    "prompt_template_path = \"prompt_template.txt\"\n",
    "\n",
    "with open(prompt_template_path, \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test2.nogit.csv')\n",
    "# Create a list to store the text lines\n",
    "text_lines = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    if(index > 200):\n",
    "        break\n",
    "    action = \"threw\" if row['classification'] != \"info\" else \"output\"\n",
    "    text_line = f\"LOGID-{index} {row['timestamp']} application: {row['app_name']} in namespace: {row['namespace_name']} {action} {row['classification']}: {row['message']}\"\n",
    "    text_lines.append(text_line)\n",
    "\n",
    "# Print the first 5 lines of text_lines\n",
    "for line in text_lines[:5]:\n",
    "    print(line)\n",
    "\n",
    "chat = prompt_template.format(\n",
    "                log_type=\"application\",\n",
    "                logs=text_lines,\n",
    "                model_schema=LogAnalysis.model_json_schema(),\n",
    "                stress_prompt=\"\"\"You are a computer security intern that's really stressed out. \n",
    "                \n",
    "                Use \"um\" and \"ah\" a lot.\"\"\",\n",
    "            )\n",
    "\n",
    "input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "print(input_tokens)\n",
    "# generate output tokens\n",
    "output = model.generate(**input_tokens, \n",
    "                        max_new_tokens=100)\n",
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "# print output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|start_of_role|>user<|end_of_role|>You are a personal assistant converting a sentence into structured command.\\n\\nYour task is to:\\n1. Analyze an english sentence\\n2. Extract (as acurately as possible) the command from the sentence. A fixed list of all possible commands to guess is described below.\\n3. Extract the parameters and their values from the sentence. A fixed list of all possible parameters is decribed below.\\n\\nHere is the list of all possible commands: \\n- csvlogs\\n- kafkalogs\\n- classifylogs\\n- Summarizelogs\\n\\nHere is the list of all possible parameters:\\n- logduration\\n- filepath\\n\\nIn your output you should also provide the confidence score for the guess of the command and it parameters.\\n\\nHere\\'s the english sentence to analyze: \"csvlogs for the last 24 hours.\"\\n\\nYou should return valid JSON in the schema\\n{\\'$defs\\': {\\'CommandParameter\\': {\\'properties\\': {\\'name\\': {\\'anyOf\\': [{\\'type\\':\\'string\\'}, {\\'type\\': \\'null\\'}], \\'title\\': \\'Name\\'}, \\'value\\': {\\'anyOf\\': [{\\'type\\':\\'string\\'}, {\\'type\\': \\'null\\'}], \\'title\\': \\'Value\\'}},\\'required\\': [\\'name\\', \\'value\\'], \\'title\\': \\'CommandParameter\\', \\'type\\': \\'object\\'}}, \\'properties\\': {\\'command\\': {\\'title\\': \\'Command\\', \\'type\\':\\'string\\'}, \\'parameters\\': {\\'anyOf\\': [{\\'items\\': {\\'$ref\\': \\'#/$defs/CommandParameter\\'}, \\'type\\': \\'array\\'}, {\\'type\\': \\'null\\'}], \\'title\\': \\'Parameters\\'}, \\'confidence_score\\': {\\'description\\': \\'Confidence score between 0 and 1\\',\\'maximum\\': 1.0,\\'minimum\\': 0.0, \\'title\\': \\'Confidence Score\\', \\'type\\': \\'number\\'}},\\'required\\': [\\'command\\', \\'parameters\\', \\'confidence_score\\'], \\'title\\': \\'SentenceAnalysis\\', \\'type\\': \\'object\\'}<|end_of_text|>\\n{\\'CommandParameter\\': {\\'name\\': \\'logduration\\', \\'value\\': \\'24hrs\\'}}<|end_of_text|>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.0-1b-a400m-instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_path = \"ibm-granite/granite-3.1-3b-a800m-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# drop device_map if running on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "model.eval()\n",
    "\n",
    "english_command = \"csvlogs for the last 24 hours.\"\n",
    "# chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# tokenize the text\n",
    "\n",
    "prompt_template_path = \"get_intended_command_prompt_template.txt\"\n",
    "\n",
    "with open(prompt_template_path, \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "                sentence=english_command,\n",
    "                model_schema=SentenceAnalysis.model_json_schema(),\n",
    "            )\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "# # print (chat)\n",
    "input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "# generate output tokens\n",
    "output = model.generate(**input_tokens, \n",
    "                        max_new_tokens=100)\n",
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "# print output\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from jsonformer import Jsonformer\n",
    "# builder = Jsonformer(\n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "#     json_schema=SentenceAnalysis.model_json_schema(),  \n",
    "#     prompt=chat,\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# print(\"Generating...\")\n",
    "# output=builder()\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moutlines\u001b[39;00m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m/workspaces/ai-lab/models/ibm-granite.granite-3.1-1b-a400m-base.Q8_0.gguf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43moutlines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllamacpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m JSON_SCHEMA = \u001b[33m'''\u001b[39m\u001b[33m{\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m{\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \u001b[33m    }\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33m}\u001b[39m\u001b[33m'''\u001b[39m\n\u001b[32m     42\u001b[39m generator = outlines.generate.json(model, JSON_SCHEMA)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ai-lab/myenv/lib/python3.11/site-packages/outlines/models/llamacpp.py:386\u001b[39m, in \u001b[36mllamacpp\u001b[39m\u001b[34m(repo_id, filename, **llamacpp_model_params)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllamacpp\u001b[39m(\n\u001b[32m    366\u001b[39m     repo_id: \u001b[38;5;28mstr\u001b[39m, filename: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, **llamacpp_model_params\n\u001b[32m    367\u001b[39m ) -> LlamaCpp:\n\u001b[32m    368\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a model from the `llama-cpp-python` library.\u001b[39;00m\n\u001b[32m    369\u001b[39m \n\u001b[32m    370\u001b[39m \u001b[33;03m    We use the `Llama.from_pretrained` classmethod that downloads models\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    384\u001b[39m \n\u001b[32m    385\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m    388\u001b[39m     \u001b[38;5;66;03m# Default to using the model's full context length\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mn_ctx\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m llamacpp_model_params:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "import outlines\n",
    "\n",
    "model_path = \"/workspaces/ai-lab/models/ibm-granite.granite-3.1-1b-a400m-base.Q8_0.gguf\"\n",
    "\n",
    "model = outlines.models.llamacpp(model_path)\n",
    "\n",
    "JSON_SCHEMA = '''{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\n",
    "            \"title\": \"Name\",\n",
    "            \"maxLength\": 10,\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"age\": {\n",
    "            \"title\": \"Age\",\n",
    "            \"type\": \"integer\"\n",
    "        },\n",
    "        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n",
    "        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n",
    "        \"strength\": {\n",
    "            \"title\": \"Strength\",\n",
    "            \"type\": \"integer\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"armor\", \"weapon\"],\n",
    "    \"definitions\": {\n",
    "        \"Armor\": {\n",
    "            \"title\": \"Armor\",\n",
    "            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"Weapon\": {\n",
    "            \"title\": \"Weapon\",\n",
    "            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    }\n",
    "}'''\n",
    "\n",
    "\n",
    "generator = outlines.generate.json(model, JSON_SCHEMA)\n",
    "character = generator(\"Give me a character description\")\n",
    "\n",
    "print(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ai-lab/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command='summarizelogs' followup='None' time_duration='24h' file='/tmp/kidan-787e39f1-dda0-[email protected]./__logs/analyzed/bd13b6dc_20250313.csv' confidence_score=0.6861607789978382\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from outlines import models, generate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "model_path=\"ibm-granite/granite-3.1-1b-a400m-base\"\n",
    "# model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "# model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_path = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, input_tokens=2048)\n",
    "model = models.Transformers(llm, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CommandParameter(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "class SentenceAnalysis(BaseModel):\n",
    "    command: str\n",
    "    followup: str\n",
    "    time_duration: str\n",
    "    file: str\n",
    "    confidence_score: float\n",
    "\n",
    "\n",
    "\n",
    "english_command = \"summarize last 24hr logs\"\n",
    "prompt_template_path = \"get_intended_command_prompt_template.txt\"\n",
    "with open(prompt_template_path, \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "prompt = prompt_template.format(\n",
    "                query=english_command,\n",
    "                model_schema=SentenceAnalysis.model_json_schema(),\n",
    "            )\n",
    "# print(prompt)\n",
    "# prompt = english_command\n",
    "\n",
    "# time_regex_pattern = r\"(0?[1-9]|1[0-2]):[0-5]\\d\\s?(am|pm)?\"\n",
    "generator = generate.json(model, SentenceAnalysis)\n",
    "\n",
    "# output = generator(\"The the best time to visit a dentist is at \")\n",
    "output = generator(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This instruction is a simple text-based task that mimics the behavior of a chatbot or AI assistant. The AI would recognize the pattern of a cow making a sound and respond accordingly. Here's how the AI might handle this:\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "User: the cow said mooo\n",
      "\n",
      "AI: Indeed, cows are known for their distinctive \"moo\" sound. It's a way for them to communicate with each other.\n",
      "\n",
      "```"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"hf-inference\",\n",
    "\tapi_key=\"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\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"the cow said mooo\"\n",
    "\t},\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "\tmodel=\"microsoft/Phi-3-mini-4k-instruct\", \n",
    "\tmessages=messages, \n",
    "\ttemperature=0.5,\n",
    "\tmax_tokens=2048,\n",
    "\ttop_p=0.7,\n",
    "\tstream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
