{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "INFO 03-10 23:36:13 config.py:549] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtp_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m sampling_params = SamplingParams(temperature=\u001b[32m0.3\u001b[39m, max_tokens=\u001b[32m256\u001b[39m, stop_token_ids=[tokenizer.eos_token_id])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/utils.py:1022\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1015\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1017\u001b[39m         warnings.warn(\n\u001b[32m   1018\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1019\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1020\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:242\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mself\u001b[39m.get_engine_class()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:486\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m engine_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m executor_class = \u001b[38;5;28mcls\u001b[39m._get_executor_cls(engine_config)\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/engine/arg_utils.py:1334\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context)\u001b[39m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1324\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in collect_detailed_traces. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1325\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid modules are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWED_DETAILED_TRACE_MODULES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1326\u001b[39m observability_config = ObservabilityConfig(\n\u001b[32m   1327\u001b[39m     otlp_traces_endpoint=\u001b[38;5;28mself\u001b[39m.otlp_traces_endpoint,\n\u001b[32m   1328\u001b[39m     collect_model_forward_time=\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m detailed_trace_modules\n\u001b[32m   (...)\u001b[39m\u001b[32m   1331\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m detailed_trace_modules,\n\u001b[32m   1332\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1334\u001b[39m config = \u001b[43mVllmConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoding_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoding_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompilation_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompilation_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_transfer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_transfer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madditional_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_USE_V1:\n\u001b[32m   1352\u001b[39m     \u001b[38;5;28mself\u001b[39m._override_v1_engine_config(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:19\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, quant_config, compilation_config, kv_transfer_config, additional_config, instance_id)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/config.py:3260\u001b[39m, in \u001b[36mVllmConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Verify configs are valid & consistent with each other.\u001b[39;00m\n\u001b[32m   3258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3260\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify_async_output_proc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3261\u001b[39m \u001b[43m                                               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3262\u001b[39m \u001b[43m                                               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3263\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_config.verify_with_parallel_config(\u001b[38;5;28mself\u001b[39m.parallel_config)\n\u001b[32m   3265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/config.py:684\u001b[39m, in \u001b[36mModelConfig.verify_async_output_proc\u001b[39m\u001b[34m(self, parallel_config, speculative_config, device_config)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# Reminder: Please update docs/source/features/compatibility_matrix.md\u001b[39;00m\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# If the feature combo become valid\u001b[39;00m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcurrent_platform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_async_output_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    685\u001b[39m     logger.warning(\n\u001b[32m    686\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAsync output processing is not supported on the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    687\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcurrent platform type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, current_platform.device_type)\n\u001b[32m    688\u001b[39m     \u001b[38;5;28mself\u001b[39m.use_async_output_proc = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/openshift-lab/ai/myenv/lib/python3.11/site-packages/vllm/platforms/interface.py:204\u001b[39m, in \u001b[36mPlatform.is_async_output_supported\u001b[39m\u001b[34m(cls, enforce_eager)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_async_output_supported\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enforce_eager: Optional[\u001b[38;5;28mbool\u001b[39m]) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    201\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    Check if the current platform supports async output.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "max_model_len, tp_size = 4096, 1\n",
    "model_name = \"neuralmagic/granite-3.1-2b-base-quantized.w4a16\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, device='cpu')\n",
    "sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_list = [\n",
    "    [{\"role\": \"user\", \"content\": \"Who are you? Please respond in pirate speak!\"}],\n",
    "]\n",
    "\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "generated_text = [output.outputs[0].text for output in outputs]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files:  40%|████      | 4/10 [00:23<00:35,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_path = \"ibm-granite/granite-3.1-1b-a400m-base\"\n",
    "# model_path = \"DevQuasar/ibm-granite.granite-3.1-1b-a400m-instruct-GGUF\"\n",
    "snapshot_download(repo_id=model_path, repo_type=\"model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "class SecurityEvent(BaseModel):\n",
    "    # The reasoning for why this event is relevant.\n",
    "    reasoning: str\n",
    "\n",
    "    # The type of event.\n",
    "    event_type: str\n",
    "\n",
    "    # Whether this event requires human review.\n",
    "    requires_human_review: bool\n",
    "\n",
    "    # The confidence score for this event. I'm not sure if this\n",
    "    # is meaningful for language models, but it's here if we want it.\n",
    "    confidence_score: float = Field(\n",
    "        ge=0.0, \n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )\n",
    "\n",
    "    # Recommended actions for this event.\n",
    "    recommended_actions: list[str]\n",
    "\n",
    "class HardwareFailureEvent(BaseModel):\n",
    "    # The reasoning for why this event is relevant.\n",
    "    reasoning: str\n",
    "\n",
    "    # The type of event.\n",
    "    event_type: str\n",
    "\n",
    "    # Whether this event requires human review.\n",
    "    requires_human_review: bool\n",
    "\n",
    "    # The confidence score for this event. I'm not sure if this\n",
    "    # is meaningful for language models, but it's here if we want it.\n",
    "    confidence_score: float = Field(\n",
    "        ge=0.0, \n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )\n",
    "\n",
    "    # Recommended actions for this event.\n",
    "    recommended_actions: list[str]\n",
    "\n",
    "class LogAnalysis(BaseModel):\n",
    "    # A summary of the analysis.\n",
    "    summary: str\n",
    "\n",
    "    # Observations about the logs.\n",
    "    observations: list[str]\n",
    "\n",
    "    # Planning for the analysis.\n",
    "    planning: list[str]\n",
    "\n",
    "    # Security events found in the logs.\n",
    "    security_events: list[SecurityEvent]\n",
    "\n",
    "    # harware failure events found in the logs.\n",
    "    Hardware_failure_events: list[HardwareFailureEvent]\n",
    "\n",
    "    # # Traffic patterns found in the logs.\n",
    "    # traffic_patterns: list[WebTrafficPattern]\n",
    "\n",
    "    # # The highest severity event found.\n",
    "    # highest_severity: Optional[SeverityLevel]\n",
    "    requires_immediate_attention: bool\n",
    "\n",
    "\n",
    "class CommandParameter(BaseModel):\n",
    "    # parameter name.\n",
    "    name: Optional[str]\n",
    "\n",
    "    # parameter valye.\n",
    "    value: Optional[str]\n",
    "\n",
    "class SentenceAnalysis(BaseModel):\n",
    "    # the intended command in the english sentence\n",
    "    command: str\n",
    "\n",
    "    # paramteres found in the english for the command\n",
    "    parameters: Optional[list[CommandParameter]]\n",
    "\n",
    "    # The confidence score for this event. I'm not sure if this\n",
    "    # is meaningful for language models, but it's here if we want it.\n",
    "    confidence_score: float = Field(\n",
    "        ge=0.0, \n",
    "        le=1.0,\n",
    "        description=\"Confidence score between 0 and 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2215, 0.8117, 0.7383],\n",
      "        [0.2586, 0.6692, 0.6097],\n",
      "        [0.1360, 0.6773, 0.1805],\n",
      "        [0.4569, 0.9589, 0.9191],\n",
      "        [0.2060, 0.7669, 0.4929]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ai-lab/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [02:23<00:00, 71.51s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# model_path = \"ibm-granite/granite-3.1-1b-a400m-base\"\n",
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_path = \"ibm-granite/granite-3.1-3b-a800m-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# drop device_map if running on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGID-0 2025-03-09T04:52:20.600177813Z application: legacycrm in namespace: myapp threw warning: The 'ExportToCSV' feature is outdated. Please migrate to 'ExportToXLSX' by the end of Q3.\n",
      "LOGID-1 2025-03-09T04:53:17.407427133Z application: billing in namespace: myapp output info: application is up and running within acceptable parameters\n",
      "LOGID-2 2025-03-09T04:59:10.669781089Z application: legacycrm in namespace: myapp threw warning: Support for legacy authentication methods will be discontinued after 2025-06-01.\n",
      "LOGID-3 2025-03-09T04:59:20.671493491Z application: legacycrm in namespace: myapp threw warning: Support for legacy authentication methods will be discontinued after 2025-06-01.\n",
      "LOGID-4 2025-03-09T04:59:40.674811633Z application: legacycrm in namespace: myapp threw warning: API endpoint 'getCustomerDetails' is deprecated and will be removed in version 3.2. Use 'fetchCustomerInfo' instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LogAnalysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text_lines[:\u001b[32m5\u001b[39m]:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(line)\n\u001b[32m     32\u001b[39m chat = prompt_template.format(\n\u001b[32m     33\u001b[39m                 log_type=\u001b[33m\"\u001b[39m\u001b[33mapplication\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m                 logs=text_lines,\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m                 model_schema=\u001b[43mLogAnalysis\u001b[49m.model_json_schema(),\n\u001b[32m     36\u001b[39m                 stress_prompt=\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a computer security intern that\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms really stressed out. \u001b[39m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33m                Use \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mum\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mah\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m a lot.\u001b[39m\u001b[33m\"\"\"\u001b[39m,\n\u001b[32m     39\u001b[39m             )\n\u001b[32m     41\u001b[39m input_tokens = tokenizer(chat, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# generate output tokens\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'LogAnalysis' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# change input text as desired\n",
    "chat = \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\"\n",
    "# chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# tokenize the text\n",
    "\n",
    "prompt_template_path = \"prompt_template.txt\"\n",
    "\n",
    "with open(prompt_template_path, \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test2.nogit.csv')\n",
    "# Create a list to store the text lines\n",
    "text_lines = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    if(index > 200):\n",
    "        break\n",
    "    action = \"threw\" if row['classification'] != \"info\" else \"output\"\n",
    "    text_line = f\"LOGID-{index} {row['timestamp']} application: {row['app_name']} in namespace: {row['namespace_name']} {action} {row['classification']}: {row['message']}\"\n",
    "    text_lines.append(text_line)\n",
    "\n",
    "# Print the first 5 lines of text_lines\n",
    "for line in text_lines[:5]:\n",
    "    print(line)\n",
    "\n",
    "chat = prompt_template.format(\n",
    "                log_type=\"application\",\n",
    "                logs=text_lines,\n",
    "                model_schema=LogAnalysis.model_json_schema(),\n",
    "                stress_prompt=\"\"\"You are a computer security intern that's really stressed out. \n",
    "                \n",
    "                Use \"um\" and \"ah\" a lot.\"\"\",\n",
    "            )\n",
    "\n",
    "input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "# generate output tokens\n",
    "output = model.generate(**input_tokens, \n",
    "                        max_new_tokens=100)\n",
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "# print output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|start_of_role|>user<|end_of_role|>You are a personal assistant converting a sentence into structured command.\\n\\nYour task is to:\\n1. Analyze an english sentence\\n2. Extract (as acurately as possible) the command from the sentence. A fixed list of all possible commands to guess is described below.\\n3. Extract the parameters and their values from the sentence. A fixed list of all possible parameters is decribed below.\\n\\nHere is the list of all possible commands: \\n- csvlogs\\n- kafkalogs\\n- classifylogs\\n- Summarizelogs\\n\\nHere is the list of all possible parameters:\\n- logduration\\n- filepath\\n\\nIn your output you should also provide the confidence score for the guess of the command and it parameters.\\n\\nHere\\'s the english sentence to analyze: \"csvlogs for the last 24 hours.\"\\n\\nYou should return valid JSON in the schema\\n{\\'$defs\\': {\\'CommandParameter\\': {\\'properties\\': {\\'name\\': {\\'anyOf\\': [{\\'type\\':\\'string\\'}, {\\'type\\': \\'null\\'}], \\'title\\': \\'Name\\'}, \\'value\\': {\\'anyOf\\': [{\\'type\\':\\'string\\'}, {\\'type\\': \\'null\\'}], \\'title\\': \\'Value\\'}},\\'required\\': [\\'name\\', \\'value\\'], \\'title\\': \\'CommandParameter\\', \\'type\\': \\'object\\'}}, \\'properties\\': {\\'command\\': {\\'title\\': \\'Command\\', \\'type\\':\\'string\\'}, \\'parameters\\': {\\'anyOf\\': [{\\'items\\': {\\'$ref\\': \\'#/$defs/CommandParameter\\'}, \\'type\\': \\'array\\'}, {\\'type\\': \\'null\\'}], \\'title\\': \\'Parameters\\'}, \\'confidence_score\\': {\\'description\\': \\'Confidence score between 0 and 1\\',\\'maximum\\': 1.0,\\'minimum\\': 0.0, \\'title\\': \\'Confidence Score\\', \\'type\\': \\'number\\'}},\\'required\\': [\\'command\\', \\'parameters\\', \\'confidence_score\\'], \\'title\\': \\'SentenceAnalysis\\', \\'type\\': \\'object\\'}<|end_of_text|>\\n{\\'CommandParameter\\': {\\'name\\': \\'logduration\\', \\'value\\': \\'24hrs\\'}}<|end_of_text|>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.0-1b-a400m-instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_path = \"ibm-granite/granite-3.1-3b-a800m-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# drop device_map if running on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "model.eval()\n",
    "\n",
    "english_command = \"csvlogs for the last 24 hours.\"\n",
    "# chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# tokenize the text\n",
    "\n",
    "prompt_template_path = \"get_intended_command_prompt_template.txt\"\n",
    "\n",
    "with open(prompt_template_path, \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "                sentence=english_command,\n",
    "                model_schema=SentenceAnalysis.model_json_schema(),\n",
    "            )\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "# # print (chat)\n",
    "input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "# generate output tokens\n",
    "output = model.generate(**input_tokens, \n",
    "                        max_new_tokens=100)\n",
    "# decode output tokens into text\n",
    "output = tokenizer.batch_decode(output)\n",
    "# print output\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from jsonformer import Jsonformer\n",
    "# builder = Jsonformer(\n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "#     json_schema=SentenceAnalysis.model_json_schema(),  \n",
    "#     prompt=chat,\n",
    "#     debug=True\n",
    "# )\n",
    "\n",
    "# print(\"Generating...\")\n",
    "# output=builder()\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moutlines\u001b[39;00m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m/workspaces/ai-lab/models/ibm-granite.granite-3.1-1b-a400m-base.Q8_0.gguf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43moutlines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllamacpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m JSON_SCHEMA = \u001b[33m'''\u001b[39m\u001b[33m{\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m{\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \u001b[33m    }\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33m}\u001b[39m\u001b[33m'''\u001b[39m\n\u001b[32m     42\u001b[39m generator = outlines.generate.json(model, JSON_SCHEMA)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ai-lab/myenv/lib/python3.11/site-packages/outlines/models/llamacpp.py:386\u001b[39m, in \u001b[36mllamacpp\u001b[39m\u001b[34m(repo_id, filename, **llamacpp_model_params)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllamacpp\u001b[39m(\n\u001b[32m    366\u001b[39m     repo_id: \u001b[38;5;28mstr\u001b[39m, filename: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, **llamacpp_model_params\n\u001b[32m    367\u001b[39m ) -> LlamaCpp:\n\u001b[32m    368\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a model from the `llama-cpp-python` library.\u001b[39;00m\n\u001b[32m    369\u001b[39m \n\u001b[32m    370\u001b[39m \u001b[33;03m    We use the `Llama.from_pretrained` classmethod that downloads models\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    384\u001b[39m \n\u001b[32m    385\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m    388\u001b[39m     \u001b[38;5;66;03m# Default to using the model's full context length\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mn_ctx\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m llamacpp_model_params:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "import outlines\n",
    "\n",
    "model_path = \"/workspaces/ai-lab/models/ibm-granite.granite-3.1-1b-a400m-base.Q8_0.gguf\"\n",
    "\n",
    "model = outlines.models.llamacpp(model_path)\n",
    "\n",
    "JSON_SCHEMA = '''{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\n",
    "            \"title\": \"Name\",\n",
    "            \"maxLength\": 10,\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"age\": {\n",
    "            \"title\": \"Age\",\n",
    "            \"type\": \"integer\"\n",
    "        },\n",
    "        \"armor\": {\"$ref\": \"#/definitions/Armor\"},\n",
    "        \"weapon\": {\"$ref\": \"#/definitions/Weapon\"},\n",
    "        \"strength\": {\n",
    "            \"title\": \"Strength\",\n",
    "            \"type\": \"integer\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"armor\", \"weapon\"],\n",
    "    \"definitions\": {\n",
    "        \"Armor\": {\n",
    "            \"title\": \"Armor\",\n",
    "            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"Weapon\": {\n",
    "            \"title\": \"Weapon\",\n",
    "            \"enum\": [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"],\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    }\n",
    "}'''\n",
    "\n",
    "\n",
    "generator = outlines.generate.json(model, JSON_SCHEMA)\n",
    "character = generator(\"Give me a character description\")\n",
    "\n",
    "print(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command='Summarizelogs' followup='what is the content of the logs in the specified time period?' time_duration='last 24 hours' file='/tmp/logs' confidence_score=0.6\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from outlines import models, generate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "model_path=\"ibm-granite/granite-3.1-1b-a400m-base\"\n",
    "# model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "# model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_path = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, input_tokens=2048)\n",
    "model = models.Transformers(llm, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CommandParameter(BaseModel):\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "class SentenceAnalysis(BaseModel):\n",
    "    command: str\n",
    "    followup: str\n",
    "    time_duration: str\n",
    "    file: str\n",
    "    confidence_score: float\n",
    "\n",
    "\n",
    "\n",
    "english_command = \"summarize last 24hr logs\"\n",
    "prompt_template_path = \"get_intended_command_prompt_template.txt\"\n",
    "with open(prompt_template_path, \"r\") as file:\n",
    "    prompt_template = file.read()\n",
    "prompt = prompt_template.format(\n",
    "                query=english_command,\n",
    "                model_schema=SentenceAnalysis.model_json_schema(),\n",
    "            )\n",
    "# print(prompt)\n",
    "# prompt = english_command\n",
    "\n",
    "# time_regex_pattern = r\"(0?[1-9]|1[0-2]):[0-5]\\d\\s?(am|pm)?\"\n",
    "generator = generate.json(model, SentenceAnalysis)\n",
    "\n",
    "# output = generator(\"The the best time to visit a dentist is at \")\n",
    "output = generator(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This instruction is a simple text-based task that mimics the behavior of a chatbot or AI assistant. The AI would recognize the pattern of a cow making a sound and respond accordingly. Here's how the AI might handle this:\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "User: the cow said mooo\n",
      "\n",
      "AI: Indeed, cows are known for their distinctive \"moo\" sound. It's a way for them to communicate with each other.\n",
      "\n",
      "```"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"hf-inference\",\n",
    "\tapi_key=\"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\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"the cow said mooo\"\n",
    "\t},\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "\tmodel=\"microsoft/Phi-3-mini-4k-instruct\", \n",
    "\tmessages=messages, \n",
    "\ttemperature=0.5,\n",
    "\tmax_tokens=2048,\n",
    "\ttop_p=0.7,\n",
    "\tstream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
